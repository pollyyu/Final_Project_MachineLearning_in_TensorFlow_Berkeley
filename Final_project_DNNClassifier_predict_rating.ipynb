{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "##### Paulynn Yu ############################################\n",
    "##### Student ID: X110000 ###################################\n",
    "##### Machine Learning in Tensor Flow #######################\n",
    "##### Final Project #########################################\n",
    "##### Applying Wide, Deep and Wide & Deep Learning Models ###\n",
    "#############################################################\n",
    "\n",
    "# import packages\n",
    "import numpy as np\n",
    "from pandas import read_csv\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from absl import flags\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# set learning rate\n",
    "learning_rate = 0.00000001\n",
    "\n",
    "# download csv and define path to folder\n",
    "dataset = \"epi_r.csv\"\n",
    "datapath = \".documents/ml_tf/final_project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_and_clean_file(dataset, datapath):\n",
    "#     set working directory and download csv file; save as data\n",
    "    os.chdir(datapath)\n",
    "    data = read_csv(dataset)\n",
    "\n",
    "    # clean and order file\n",
    "    data.dropna(inplace=True) # remove NAs\n",
    "    data.reset_index(inplace=True) # resetting index\n",
    "    data.drop('index', axis='columns', inplace=True)\n",
    "\n",
    "    return(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "\n",
    "    def input_columns(dataset,datapath):\n",
    "        data = download_and_clean_file(dataset,datapath)\n",
    "\n",
    "        # Preparing columns\n",
    "        data.head(1)              # reads the first line\n",
    "        rows = len(data)          # counts the number of rows in the file\n",
    "        shape = data.shape        # shows the shape\n",
    "\n",
    "\n",
    "        features = []\n",
    "        label = []\n",
    "\n",
    "        rating = data['rating'] # label\n",
    "\n",
    "        calories = data['calories']\n",
    "        protein = data['protein']\n",
    "        fat = data['fat']\n",
    "        sodium = data['sodium']\n",
    "\n",
    "        dessert = data['dessert']\n",
    "        peanut_free = data['peanut free']\n",
    "        soy_free = data['soy free']\n",
    "        tree_nut_free = data['tree nut free']\n",
    "        vegetarian = data['vegetarian']\n",
    "        gourmet = data['gourmet']\n",
    "        kosher = data['kosher']\n",
    "        pescatarian = data['pescatarian']\n",
    "        quick_easy = data['quick & easy']\n",
    "        wheat_gluten_free = data['wheat/gluten-free']\n",
    "        bake = data['bake']\n",
    "        summer = data['summer']\n",
    "        dairy_free = data['dairy free']\n",
    "        side = data['side']\n",
    "        no_sugar_added = data['no sugar added']\n",
    "        winter = data['winter']\n",
    "        fall = data['fall']\n",
    "        dinner = data['dinner']\n",
    "        sugar_conscious = data['sugar conscious']\n",
    "        healthy = data['healthy']\n",
    "        kidney_friendly = data['kidney friendly']\n",
    "        onion = data['onion']\n",
    "        tomato = data['tomato']\n",
    "        vegetable = data['vegetable']\n",
    "        milk_cream = data['milk/cream']\n",
    "        fruit = data['fruit']\n",
    "        vegan = data['vegan']\n",
    "        kid_friendly = data['kid-friendly']\n",
    "        egg = data['egg']\n",
    "        spring = data['spring']\n",
    "        herb = data['herb']\n",
    "        garlic = data['garlic']\n",
    "        salad = data['salad']\n",
    "        dairy = data['dairy']\n",
    "        thanksgiving = data['thanksgiving']\n",
    "        appetizer = data['appetizer']\n",
    "        lunch = data['lunch']\n",
    "        cheese = data['cheese']\n",
    "        chicken = data['chicken']\n",
    "        roast = data['roast']\n",
    "        no_cook = data['no-cook']\n",
    "        soup_stew = data['soup/stew']\n",
    "        cocktail_party = data['cocktail party']\n",
    "        ginger = data['ginger']\n",
    "        potato = data['potato']\n",
    "        chill = data['chill']\n",
    "        grill_barbecue = data['grill/barbecue']\n",
    "        lemon = data['lemon']\n",
    "        drink = data['drink']\n",
    "        sauce = data['sauce']\n",
    "        low_cal = data['low cal']\n",
    "        christmas = data['christmas']\n",
    "        high_fiber = data['high fiber']\n",
    "        food_processor = data['food processor']\n",
    "\n",
    "        for k in range(rows): # use loop to put it in the expected format\n",
    "            # appending features\n",
    "            features.append([calories[k],protein[k], fat[k], sodium[k], peanut_free[k],soy_free[k],tree_nut_free[k], \n",
    "            vegetarian[k],gourmet[k], kosher[k], pescatarian[k], quick_easy[k], wheat_gluten_free[k], bake[k], summer[k], \n",
    "            dessert[k], dairy_free[k],side[k], no_sugar_added[k], winter[k], fall[k], dinner[k], sugar_conscious[k], \n",
    "            healthy[k], kidney_friendly[k], onion[k], tomato[k], vegetable[k], milk_cream[k], fruit[k], vegan[k], \n",
    "            kid_friendly[k],egg[k], spring[k], herb[k], garlic[k], salad[k], dairy[k], thanksgiving[k], appetizer[k], lunch[k],\n",
    "            cheese[k], chicken[k], roast[k], no_cook[k], soup_stew[k], cocktail_party[k], ginger[k], potato[k],\n",
    "            chill[k], grill_barbecue[k], lemon[k], drink[k], sauce[k], low_cal[k], christmas[k], high_fiber[k], food_processor[k]])\n",
    "\n",
    "            # creating classes for labels into 5 buckets, i.e less than 1 star rating is class 0, between 1 & 2 star rating is class 1\n",
    "            if rating[k] <= 1:\n",
    "                label.append(0)\n",
    "            elif rating[k]<=2:\n",
    "                label.append(1)\n",
    "            elif rating[k]<=3:\n",
    "                label.append(2)\n",
    "            elif rating[k]<=4:\n",
    "                label.append(3)\n",
    "            else: label.append(4)\n",
    "\n",
    "        return np.array(label), np.array(features)\n",
    "\n",
    "    def input_data(dataset,datapath):\n",
    "    # splitting data 70% train 30% test\n",
    "        label, features = input_columns(dataset,datapath)\n",
    "        train_len = int(len(features) * 0.7)\n",
    "        train_label, train_data = label[:train_len], features[:train_len]\n",
    "        test_label, test_data = label[train_len:], features[train_len:]\n",
    "        return train_label, train_data, test_label, test_data\n",
    "\n",
    "    def build_estimator(model_type, model_dir):\n",
    "    # Build 3 layer DNN with 100, 75, 50, 25 units respectively.\n",
    "        hidden_units = [10, 20, 50,]\n",
    "\n",
    "        feature_columns = [tf.feature_column.numeric_column(\"x\",shape=[58])]\n",
    "        deep_columns = [tf.feature_column.numeric_column(\"deep\",shape=[4])]\n",
    "        wide_columns = [tf.feature_column.numeric_column(\"wide\",shape=[54])]\n",
    "\n",
    "        if model_type == 'wide':\n",
    "            return tf.estimator.LinearClassifier(feature_columns=feature_columns,\n",
    "                                          n_classes=5,\n",
    "                                          model_dir=model_dir)\n",
    "\n",
    "        elif model_type == 'deep':\n",
    "            return tf.estimator.DNNClassifier(feature_columns=feature_columns,\n",
    "                                          n_classes=5,\n",
    "                                          hidden_units = hidden_units,\n",
    "                                          model_dir=model_dir)\n",
    "\n",
    "        else:\n",
    "            return tf.estimator.DNNLinearCombinedClassifier(\n",
    "                                              n_classes=5,\n",
    "                                              linear_feature_columns=wide_columns,\n",
    "                                              dnn_feature_columns=deep_columns,\n",
    "                                              dnn_hidden_units = hidden_units,\n",
    "                                              model_dir=model_dir)\n",
    "\n",
    "    def train_input(model_type):\n",
    "        train_label, train_data, test_label, test_data = input_data(dataset,datapath)\n",
    "        if model_type == 'wide' or model_type == 'deep':\n",
    "            return tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": np.array(train_data)},\n",
    "            y=np.array(train_label),\n",
    "            num_epochs=None,\n",
    "            shuffle=True)\n",
    "\n",
    "        else:\n",
    "        # Define the training inputs\n",
    "            return tf.estimator.inputs.numpy_input_fn(\n",
    "              x={\"deep\": np.array(train_data[:,0:4]), \"wide\": np.array(train_data[:,4:])},\n",
    "              y=np.array(train_label),\n",
    "            num_epochs=None,\n",
    "            shuffle=True)\n",
    "    def test_input(model_type):\n",
    "        train_label, train_data, test_label, test_data = input_data(dataset,datapath)\n",
    "        if model_type == 'wide' or model_type == 'deep':\n",
    "            return tf.estimator.inputs.numpy_input_fn(\n",
    "            x={\"x\": np.array(test_data)},\n",
    "            y=np.array(test_label),\n",
    "            num_epochs=1,\n",
    "            shuffle=True)\n",
    "\n",
    "        else:\n",
    "        # Define the training inputs\n",
    "            return tf.estimator.inputs.numpy_input_fn(\n",
    "              x={\"deep\": np.array(test_data[:,0:4]), \"wide\": np.array(test_data[:,4:])},\n",
    "              y=np.array(test_label),\n",
    "            num_epochs=1,\n",
    "            shuffle=True)\n",
    "\n",
    "    def train_model(model_type, model_dir):\n",
    "        train_label, train_data, test_label, test_data = input_data(dataset,datapath)\n",
    "        classifier = build_estimator(model_type, model_dir)\n",
    "        train_input_fn = train_input(model_type)\n",
    "\n",
    "        classifier.train(input_fn=train_input_fn, steps=8000)\n",
    "\n",
    "    def test_model_accuracy(model_type, model_dir):\n",
    "\n",
    "        train_model(model_type, model_dir)\n",
    "        train_label, train_data, test_label, test_data = input_data(dataset,datapath)\n",
    "        classifier = build_estimator(model_type, model_dir)\n",
    "        test_input_fn = test_input(model_type)\n",
    "        \n",
    "        # getting accuracy score\n",
    "        accuracy_score = classifier.evaluate(input_fn=test_input_fn)[\"accuracy\"]\n",
    "        print(\"\\nTest Accuracy: {0:f}\\n\".format(accuracy_score),\"model_type: \", model_type)\n",
    "\n",
    "        # getting predictions and plotting confusion matrix   \n",
    "        prediction = classifier.predict(input_fn=test_input_fn)\n",
    "        predicted_classes = [p[\"class_ids\"] for p in prediction]\n",
    "\n",
    "        # prepare loop to ensure format align\n",
    "        row = len(predicted_classes)\n",
    "        prediction_array =[]\n",
    "\n",
    "        for k in range(row):     # use loop to put it in the expected format\n",
    "                # features\n",
    "                prediction_array.append(int(predicted_classes[k]))\n",
    "\n",
    "        # define labels\n",
    "        labels = ['0', '1', '2', '3', '4']\n",
    "        cm = confusion_matrix(test_label, prediction_array)\n",
    "        # print(cm)\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        cax = ax.matshow(cm)\n",
    "        plt.title('Confusion matrix of the classifier')\n",
    "        fig.colorbar(cax)\n",
    "        ax.set_xticklabels([''] + labels)\n",
    "        ax.set_yticklabels([''] + labels)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.show()\n",
    "                              \n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=g) as sess:\n",
    "\n",
    "    sess.graph.as_graph_def()\n",
    "    \n",
    "    # modify model type: wide, deep, wide+deep\n",
    "    test_model_accuracy(model_type = \"wide\", model_dir = 'fv1')\n",
    "#     test_model_accuracy(model_type = \"deep\", model_dir = 'fv2')\n",
    "#     test_model_accuracy(model_type = \"wide + deep\", model_dir = 'fv3')\n",
    "    \n",
    "    merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "    # initializations\n",
    "    sess.run(init)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
